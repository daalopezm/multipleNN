{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 5.2547\n",
      "Epoch [2/25], Loss: 4.6993\n",
      "Epoch [3/25], Loss: 4.1982\n",
      "Epoch [4/25], Loss: 3.6537\n",
      "Epoch [5/25], Loss: 3.2897\n",
      "Epoch [6/25], Loss: 2.9637\n",
      "Epoch [7/25], Loss: 2.6938\n",
      "Epoch [8/25], Loss: 2.4230\n",
      "Epoch [9/25], Loss: 2.2165\n",
      "Epoch [10/25], Loss: 2.0562\n",
      "Epoch [11/25], Loss: 1.8169\n",
      "Epoch [12/25], Loss: 1.7134\n",
      "Epoch [13/25], Loss: 1.5065\n",
      "Epoch [14/25], Loss: 1.3765\n",
      "Epoch [15/25], Loss: 1.2551\n",
      "Epoch [16/25], Loss: 1.1304\n",
      "Epoch [17/25], Loss: 1.0319\n",
      "Epoch [18/25], Loss: 0.9174\n",
      "Epoch [19/25], Loss: 0.8565\n",
      "Epoch [20/25], Loss: 0.7645\n",
      "Epoch [21/25], Loss: 0.7196\n",
      "Epoch [22/25], Loss: 0.6694\n",
      "Epoch [23/25], Loss: 0.5860\n",
      "Epoch [24/25], Loss: 0.5511\n",
      "Epoch [25/25], Loss: 0.4982\n",
      "Generated Text: <bos> encoder representations from gpt generative produce human-like significant the time across domains. field model transfer with achieved training time across\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Prepare Data\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    \"Transformers have revolutionized the field of natural language processing.\",\n",
    "    \"They are widely used in tasks such as translation, summarization, and text generation.\",\n",
    "    \"Attention mechanisms allow transformers to focus on relevant parts of the input sequence.\",\n",
    "    \"The architecture consists of multiple layers of self-attention and feedforward networks.\",\n",
    "    \"Pretrained transformer models like BERT, GPT, and T5 have achieved state-of-the-art results.\",\n",
    "    \"PyTorch makes it easy to implement deep learning models, including transformers.\",\n",
    "    \"Training large transformer models requires significant computational resources.\",\n",
    "    \"The transformer model can handle long-range dependencies effectively.\",\n",
    "    \"Researchers are constantly exploring ways to improve transformer architectures.\",\n",
    "    \"Self-attention mechanisms enable parallel computation over input sequences.\",\n",
    "    \"BERT stands for Bidirectional Encoder Representations from Transformers.\",\n",
    "    \"GPT models are generative and can produce human-like text.\",\n",
    "    \"Transformers have been successfully applied to fields beyond NLP, such as vision.\",\n",
    "    \"The concept of attention was first introduced in machine translation tasks.\",\n",
    "    \"Multi-head attention allows the model to attend to different parts of the input in parallel.\",\n",
    "    \"Sequence-to-sequence models can be built using transformer encoders and decoders.\",\n",
    "    \"Transformers use positional encoding to retain the order of the input sequence.\",\n",
    "    \"Optimizing transformers requires advanced techniques like learning rate scheduling.\",\n",
    "    \"Transfer learning with transformers has significantly reduced training time for many tasks.\",\n",
    "    \"OpenAI's GPT-3 model has demonstrated remarkable text generation capabilities.\",\n",
    "    \"Transformers are known for their scalability and flexibility in handling various data types.\",\n",
    "    \"With enough data, transformers can generalize well across multiple domains.\",\n",
    "    \"Fine-tuning pretrained transformers on specific tasks yields excellent results.\",\n",
    "    \"The rise of transformers has shifted the focus from recurrent networks to attention-based models.\",\n",
    "    \"The concept of self-attention underpins the strength of the transformer architecture.\",\n",
    "    \"Large-scale transformer models require careful optimization to avoid overfitting.\",\n",
    "    \"Natural language understanding tasks like question answering have benefited from transformers.\",\n",
    "    \"Advances in transformer research continue to push the boundaries of AI and machine learning.\",\n",
    "    \"Hybrid models combining transformers with other architectures are being explored for enhanced performance.\",\n",
    "    \"Applications of transformers in speech processing and computer vision are gaining traction.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenizer: splits sentences into words by spaces\n",
    "def tokenize(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "# Build a vocabulary from the dataset\n",
    "def build_vocab(data):\n",
    "    counter = Counter()\n",
    "    for sentence in data:\n",
    "        counter.update(tokenize(sentence))\n",
    "    vocab = {word: i+4 for i, word in enumerate(counter.keys())}  # Reserve 0, 1, 2, 3 for special tokens\n",
    "    vocab[\"<unk>\"] = 0\n",
    "    vocab[\"<pad>\"] = 1\n",
    "    vocab[\"<bos>\"] = 2\n",
    "    vocab[\"<eos>\"] = 3\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(data)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Encode the data into indices\n",
    "def encode(sentence, vocab):\n",
    "    tokens = [\"<bos>\"] + tokenize(sentence) + [\"<eos>\"]\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "encoded_data = [encode(sentence, vocab) for sentence in data]\n",
    "\n",
    "# Step 2: Prepare Batches\n",
    "\n",
    "# Pad sequences to the same length\n",
    "def pad_sequence(sequences, pad_value):\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    return [seq + [pad_value] * (max_len - len(seq)) for seq in sequences]\n",
    "\n",
    "# Convert the data into batches\n",
    "def create_batches(encoded_data, batch_size, pad_value):\n",
    "    batches = []\n",
    "    for i in range(0, len(encoded_data), batch_size):\n",
    "        batch = encoded_data[i:i+batch_size]\n",
    "        batch = pad_sequence(batch, pad_value)\n",
    "        batches.append(torch.tensor(batch, dtype=torch.long).T)  # Transpose to (sequence_length, batch_size)\n",
    "    return batches\n",
    "\n",
    "batch_size = 2\n",
    "batches = create_batches(encoded_data, batch_size, vocab[\"<pad>\"])\n",
    "\n",
    "# Step 3: Define the Transformer Model\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoder = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(self.pos_encoder, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [sequence_length, batch_size]\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.embed_size, dtype=torch.float32))\n",
    "        src = self.dropout(src)\n",
    "        memory = self.transformer(src)\n",
    "        out = self.fc_out(memory)\n",
    "        return out\n",
    "\n",
    "# Step 4: Train the Model\n",
    "\n",
    "def train(model, data_batches, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_batches:\n",
    "            optimizer.zero_grad()\n",
    "            # Shift target for teacher forcing\n",
    "            input_seq = batch[:-1]  # All except the last token\n",
    "            target_seq = batch[1:]  # All except the first token\n",
    "            output = model(input_seq)\n",
    "            \n",
    "            # Use reshape instead of view to flatten the tensors\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target_seq.reshape(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(data_batches):.4f}')\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_dim, num_layers)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
    "\n",
    "# Train the model\n",
    "train(model, batches, optimizer, criterion, num_epochs)\n",
    "\n",
    "# Step 5: Generate Text\n",
    "\n",
    "def generate_text(model, start_token, max_length=20):\n",
    "    model.eval()\n",
    "    src = torch.tensor([vocab[start_token]], dtype=torch.long).unsqueeze(1)\n",
    "    generated_tokens = [start_token]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        output = model(src)\n",
    "        next_token = torch.argmax(output[-1, :], dim=1).item()\n",
    "        generated_tokens.append(list(vocab.keys())[next_token])\n",
    "        if list(vocab.keys())[next_token] == \"<eos>\":\n",
    "            break\n",
    "        src = torch.cat([src, torch.tensor([[next_token]], dtype=torch.long)], dim=0)\n",
    "    \n",
    "    return ' '.join(generated_tokens)\n",
    "\n",
    "# Generate some text\n",
    "generated_sentence = generate_text(model, \"<bos>\")\n",
    "print(\"Generated Text:\", generated_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
